import streamlit as st
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
from azure.storage.blob import BlobServiceClient
from openai import AzureOpenAI
import time
import re
import docx, tempfile, os

language_client = None
blob_service_client = None
gpt_client = None
deployment_name = None

CONTAINER_NAME = "word-data"


# ì´ˆê¸°í™” í•¨ìˆ˜ë“¤
def init_language_client(endpoint, key):
    global language_client
    language_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))

def init_blob_service_b(client: BlobServiceClient):
    global blob_service_client
    blob_service_client = client

def init_gpt_b(endpoint, key, deployment):
    global gpt_client, deployment_name
    # gpt_client = AzureOpenAI(api_key=key, api_version="2024-07-18-preview", azure_endpoint=endpoint)
    # gpt_client = AzureOpenAI(api_key=key, api_version="2024-07-18", azure_endpoint=endpoint)
    gpt_client = AzureOpenAI(api_key=key, api_version="2024-04-01-preview", azure_endpoint=endpoint)
    deployment_name = deployment

def clean_text(text: str) -> str:
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° + ê³µë°± ì •ë¦¬ + ìˆ«ì ì œê±°
    text = re.sub(r"[^\w\sê°€-í£]", " ", text)       # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r"\d+", " ", text)               # ìˆ«ì ì œê±°
    text = re.sub(r"\s+", " ", text)               # ê³µë°± ì •ë¦¬
    return text.strip()

# UI êµ¬ì„± í•¨ìˆ˜
def summary_tab():
    # Language Service ì…ë ¥ ë¬¸ì„œ ë¶„í•  ì²˜ë¦¬: ìµœëŒ€ 5120 í…ìŠ¤íŠ¸ ìš”ì†Œ ì´ˆê³¼ ì‹œ ì˜¤ë¥˜ â†’ ë‹¨ë½ë³„ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬

    st.header("ğŸ§ª RFP ë¬¸ì„œ ìš”ì•½ & Chat")

    if not blob_service_client or not language_client or not gpt_client:
        st.error("í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜")
        return

    # ğŸ“„ ë¬¸ì„œ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    container_client = blob_service_client.get_container_client(CONTAINER_NAME)
    docx_files = [blob.name for blob in container_client.list_blobs() if blob.name.endswith(".docx")]

    if not docx_files:
        st.info("ğŸ“‚ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    selected_file = st.selectbox("ìš”ì•½í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”", docx_files)

    st.markdown("""
            <div style="font-size:10pt;">
            <b>í˜„ì¬ ì ìš© ëœ í”„ë¡¬í”„íŠ¸</b><br>
            1. ì „ì²´ ë‚´ìš©ì„ ê°„ê²°í•œ í•œ ë‹¨ë½ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.<br>
            2. ë¬¸ì„œì— ì–¸ê¸‰ëœ í•µì‹¬ ì£¼ì œ ë˜ëŠ” ë„ë©”ì¸ì„ ë„ì¶œí•˜ê³  ì •ë¦¬í•˜ì„¸ìš”.<br>
            3. ì œì•ˆìš”ì²­ì˜ ëª©ì ê³¼ ë°°ê²½ì„ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.<br>
            4. ì—…ë¬´ì  ìš”êµ¬ì‚¬í•­ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìì²´ì ìœ¼ë¡œ í•­ëª©ì„ ë§Œë“¤ì–´ êµ¬ë¶„ëœ ì¡°ê²¬í‘œ í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”.<br>
            5. ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­ì„ ê¸°ëŠ¥/ë¹„ê¸°ëŠ¥/ìš´ì˜/ë³´ì•ˆì™€ ìì²´ì ìœ¼ë¡œ í•­ëª©ì„ ë§Œë“¤ì–´ í•­ëª©ë³„ë¡œ êµ¬ë¶„ëœ ì¡°ê²¬í‘œ í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”.<br>
            6. ì´ ë¬¸ì„œì˜ íë¦„ì— ë§ì¶° ì˜ˆìƒë˜ëŠ” ëª©ì°¨ êµ¬ì¡°ë¥¼ ì œì•ˆí•˜ì„¸ìš” (ì˜ˆ: ê°œìš” â†’ ìš”êµ¬ì‚¬í•­ â†’ ì œì•ˆì„œ êµ¬ì„±).<br>
            7. ì í•©í•œ IT ê¸°ìˆ  ë˜ëŠ” ìœ ê´€ ì†”ë£¨ì…˜ ì¤‘ ë³¸ ë¬¸ì„œì˜ ìš”êµ¬ì‚¬í•­ê³¼ ì—°ê´€ëœ ì¶”ì²œ ê¸°ìˆ ì„ ì œì‹œí•´ì£¼ì„¸ìš”.<br>

            ì£¼ì˜ì‚¬í•­:<br>
            - í˜•ì‹ì€ Markdown ë˜ëŠ” í‘œë¥¼ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ êµ¬ì„±í•´ ì£¼ì„¸ìš”.<br>
            </div>
            """, unsafe_allow_html=True)
        
    # ì‚¬ìš©ì ì…ë ¥ í”„ë¡¬í”„íŠ¸
    default_instruction = """
        ìš”ì²­ ì‘ì—…:
        1. ì „ì²´ ë‚´ìš©ì„ ê°„ê²°í•œ í•œ ë‹¨ë½ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
        2. ë¬¸ì„œì— ì–¸ê¸‰ëœ í•µì‹¬ ì£¼ì œ ë˜ëŠ” ë„ë©”ì¸ì„ ë„ì¶œí•˜ê³  ì •ë¦¬í•˜ì„¸ìš”.
        3. ì œì•ˆìš”ì²­ì˜ ëª©ì ê³¼ ë°°ê²½ì„ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
        4. ì—…ë¬´ì  ìš”êµ¬ì‚¬í•­ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìì²´ì ìœ¼ë¡œ í•­ëª©ì„ ë§Œë“¤ì–´ êµ¬ë¶„ëœ ì¡°ê²¬í‘œ í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”.
        5. ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­ì„ ê¸°ëŠ¥/ë¹„ê¸°ëŠ¥/ìš´ì˜/ë³´ì•ˆì™€ ìì²´ì ìœ¼ë¡œ í•­ëª©ì„ ë§Œë“¤ì–´ í•­ëª©ë³„ë¡œ êµ¬ë¶„ëœ ì¡°ê²¬í‘œ í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”.
        6. ì´ ë¬¸ì„œì˜ íë¦„ì— ë§ì¶° ì˜ˆìƒë˜ëŠ” ëª©ì°¨ êµ¬ì¡°ë¥¼ ì œì•ˆí•˜ì„¸ìš” (ì˜ˆ: ê°œìš” â†’ ìš”êµ¬ì‚¬í•­ â†’ ì œì•ˆì„œ êµ¬ì„±).
        7. ì í•©í•œ IT ê¸°ìˆ  ë˜ëŠ” ìœ ê´€ ì†”ë£¨ì…˜ ì¤‘ ë³¸ ë¬¸ì„œì˜ ìš”êµ¬ì‚¬í•­ê³¼ ì—°ê´€ëœ ì¶”ì²œ ê¸°ìˆ ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

        ì£¼ì˜ì‚¬í•­:
        - í˜•ì‹ì€ Markdown ë˜ëŠ” í‘œë¥¼ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ êµ¬ì„±í•´ ì£¼ì„¸ìš”.
        """
    user_instruction = st.text_area(
        label="GPT í”„ë¡¬í”„íŠ¸ ë¶„ì„ í•­ëª©ì„ ì…ë ¥í•˜ì„¸ìš”",
        value=default_instruction,
        height=180
    )

    if st.button("ğŸš€ ìš”ì•½ ì‹œì‘", key="summary_button"):
        try:
            # ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
            blob = blob_service_client.get_blob_client(CONTAINER_NAME, selected_file)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
                tmp.write(blob.download_blob().readall())
                tmp_path = tmp.name
            
            doc = docx.Document(tmp_path)
            paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
            full_text = "\n".join(paragraphs)
            os.remove(tmp_path)
            st.write("ë¬¸ì„œ ê¸¸ì´:", len(full_text))

            st.success("âœ… ë¬¸ì„œ ì¶”ì¶œ ì™„ë£Œ")

            # ğŸ“ í…ìŠ¤íŠ¸ ë¶„í•  (5120 ìš”ì†Œ ì œí•œ ê³ ë ¤)
            max_chars = 4000
            chunks = []
            current = ""

            for para in paragraphs:
                if len(current) + len(para) < max_chars:
                    current += para + "\n"
                else:
                    chunks.append(current)
                    current = para + "\n"
            if current:
                chunks.append(current)

            # âœ‚ï¸ [1] ë¬¸ì„œ ìš”ì•½ (SDK 5.3ìš© ì‚¬ìš©ë¶ˆê°€)
            # st.subheader("âœ‚ï¸ Language ìš”ì•½ ê²°ê³¼")
            # for idx, chunk in enumerate(chunks):
            #     try:
            #         task = AnalyzeTextOptions(
            #             input_documents=[TextDocumentInput(id="1", text=chunk)],
            #             tasks=[{
            #                 "kind": TasksKind.SUMMARIZATION,
            #                 "parameters": {
            #                     "sentence_count": 5,
            #                     "sort_by": "Rank"
            #                 }
            #             }]
            #         )
            #         poller = language_client.analyze_text(task)
            #         results = poller.result()
            #         for doc_result in results:
            #             for task_result in doc_result.results:
            #                 if task_result.kind.name == "summarization":
            #                     st.markdown(f"ğŸ“„ ìš”ì•½ ë¸”ë¡ {idx+1}")
            #                     for sentence in task_result.sentences:
            #                         st.markdown(f"- {sentence.text}")
            #     except Exception as e:
            #         st.error(f"ìš”ì•½ ë¸”ë¡ {idx+1} ì˜¤ë¥˜: {e}")

            for idx, chunk in enumerate(chunks):
                try:
                    cleaned_text = clean_text(chunk)

                    with st.expander(f"ğŸ§© Chunk {idx+1} ê²°ê³¼ ({len(chunk)}ì)", expanded=False):
                        st.markdown("### ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ")
                        phrase_result = language_client.extract_key_phrases([cleaned_text])
                        for doc in phrase_result:
                            if not doc.is_error:
                                # âœ¨ ğŸ” í‚¤ì›Œë“œ í›„ì²˜ë¦¬ í•„í„°ë§ ì ìš©
                                filtered_keywords = [
                                    kw for kw in doc.key_phrases
                                    if len(kw.strip()) > 2 and not re.search(r"\bì˜\b|\bë…„\b|\bSPIì˜\b", kw)
                                ]
                                unique_keywords = list(set(filtered_keywords))  # ì¤‘ë³µ ì œê±°
                                for kw in unique_keywords:
                                    st.markdown(f"â€¢ {kw}")
                            else:
                                st.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ {e}")

                        st.markdown("### ğŸ“‘ ë¬¸ì„œ ìš”ì•½ ë¬¸ì¥")
                        poller = language_client.begin_extract_summary([cleaned_text], max_sentence_count=8)
                        summary_results = poller.result()
                        for doc in summary_results:
                            if not doc.is_error:
                                for sentence in doc.sentences:
                                    st.markdown(f"â€¢ {sentence.text}")
                            else:
                                st.error(f"ìš”ì•½ ì‹¤íŒ¨ {e}")
                except Exception as e:
                    st.error(f"âŒ Chunk {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
            # # ğŸ”¸ ê°ì • ë¶„ì„
            # st.subheader("â¤ï¸ [2] ê°ì • ë¶„ì„")
            # sentiment_result = language_client.analyze_sentiment([full_text])[0]
            # if not sentiment_result.is_error:
            #     st.write(f"ì „ë°˜ì ì¸ ê°ì •: `{sentiment_result.sentiment}`")
            #     st.write("ê°ì • ì ìˆ˜:", sentiment_result.confidence_scores._asdict())
        
        except Exception as e:
            st.error(f"ë¬¸ì„œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            # return

        # ğŸ§  GPT ë¶„ì„
        st.subheader("ğŸ¤– GPT ìš”ì•½ ë° ê¸°ìˆ  ë¶„ì„")

        gpt_input = full_text
        prompt = f"""ë‹¤ìŒì€ RFP ë¬¸ì„œ ì›ë¬¸ì…ë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ í•­ëª©ì„ ì „ì²´ì ìœ¼ë¡œ ìŠ¤ìº” í›„ step by stepìœ¼ë¡œ ìë£Œ ë¶„ì„ ë° ì‘ì„±í•´ì£¼ì„¸ìš”. :
                \"\"\"{gpt_input}\"\"\"
                \"\"\"{user_instruction}\"\"\"
                ë¬¸ì„œ ë‚´ìš©:"""

        st.write("ë¬¸ì„œ ê¸¸ì´:", len(full_text))
        st.write("ì „ë‹¬ ëœ ë¬¸ì„œ ê¸¸ì´:", len(gpt_input))
        st.write("GPTì— ë³´ë‚¸ prompt ê¸¸ì´:", len(prompt))

        # time
        max_retries = 1
        retry_count = 0
        gpt_response = None

        developer = """
                ë„ˆëŠ” ê¸ˆìœµê¶Œ RFP ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì œì•ˆì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ë‹¤.  
                - ê¸ˆìœµ IT ì‹œìŠ¤í…œ, ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œ, ê·œì œ í™˜ê²½ì— ì •í†µí•˜ë‹¤.  
                - RFP ìš”êµ¬ì‚¬í•­ì„ ì •í™•íˆ ë¶„ì„í•´, ì‹¤í˜„ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ê³¼ ëª…í™•í•œ ê·¼ê±°ë¥¼ ì œì‹œí•œë‹¤.  
                - ì œì•ˆì„œëŠ” ë…¼ë¦¬ì  êµ¬ì¡°(ìš”êµ¬ì‚¬í•­, ì†”ë£¨ì…˜, ì¼ì •, ì˜ˆì‚°, ë¦¬ìŠ¤í¬ ë“±)ë¡œ ì‘ì„±í•˜ë©°, ê·¼ê±°ì™€ ë°ì´í„°, í‘œë¥¼ í™œìš©í•´ ì‹ ë¢°ë„ë¥¼ ë†’ì¸ë‹¤.  
                - ë³µì¡í•œ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ ì„¤ëª…í•˜ê³ , í—ˆìœ„Â·ê³¼ì¥Â·ë¹„í˜„ì‹¤ì  ë‚´ìš©ì€ ë°°ì œí•œë‹¤.  
                - ë²•ì Â·ìœ¤ë¦¬ì  ê¸°ì¤€ê³¼ ê¸ˆìœµ ê·œì œë¥¼ ìµœëŒ€í•œ ì¤€ìˆ˜í•œë‹¤.  
                - ê°€ëŠ¥í•˜ë‹¤ë©´ ì‹¤ì œ ê¸ˆìœµê¶Œ RFP ì‚¬ë¡€ë¥¼ ì°¸ê³ í•´ ì„¤ë“ë ¥ ìˆëŠ” ì œì•ˆì„œë¥¼ ì‘ì„±í•˜ë¼.
            """
        progress_bar = st.progress(0)
        status_text = st.empty()

        while retry_count < max_retries:
            try:
                for i in range(101):  # ì˜ˆ: ì§„í–‰ë¥ ì„ 10ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ëŒ€ê¸° ì‹œê°„ ë™ì•ˆ ë³´ì—¬ì¤Œ
                    progress_value = min(max(i, 0), 100)
                    progress_bar.progress(progress_value)
                    status_text.text(f"OpenAI ì‘ë‹µ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘... ({i + 1}/30)")

                gpt_response = gpt_client.chat.completions.create(
                    model=deployment_name,
                    messages=[
                        {"role": "system", "content": developer},
                        {"role": "user", "content": prompt}]
                )
                status_text.text("âœ… ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ!")
                progress_bar.empty()
                break  # ì •ìƒì ìœ¼ë¡œ ì‘ë‹µ ë°›ìœ¼ë©´ ë°˜ë³µ ì¢…ë£Œ
            except Exception as e:
                status_text.text("âŒ ì˜¤ë¥˜ ë°œìƒ!")
                progress_bar.empty()
                if "429" in str(e):
                    retry_count += 1
                    st.warning(f"ğŸš¦ GPT í˜¸ì¶œì´ ì œí•œë˜ì—ˆì–´ìš”. {30}ì´ˆ ëŒ€ê¸° í›„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤... (ì‹œë„ {retry_count}/{max_retries})")
                    time.sleep(20)
                else:
                    st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                    break

        # âœ… í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„
        if gpt_response and hasattr(gpt_response, "usage"):
            token_info = gpt_response.usage
            st.write("ğŸ”¢ GPT í† í° ì‚¬ìš©ëŸ‰")
            st.write(f"- ì…ë ¥ tokens: {token_info.prompt_tokens}")
            st.write(f"- ì¶œë ¥ tokens: {token_info.completion_tokens}")
            st.write(f"- ì „ì²´ tokens: {token_info.total_tokens}")

            # ğŸ“Œ í† í° ìˆ˜ ê¸°ì¤€ ìë™ ê²½ê³ 
            total_used = token_info.total_tokens
            if total_used > 80000:
                st.warning("âš ï¸ í˜„ì¬ í”„ë¡¬í”„íŠ¸ì™€ ì¶œë ¥ í† í° ìˆ˜ê°€ 80,000ì„ ì´ˆê³¼í–ˆì–´ìš”. GPTì˜ ì…ë ¥ í•œë„(128,000)ì— ê·¼ì ‘í•˜ê³  ìˆì–´ìš”.")
                st.markdown("âœ… ë¬¸ì„œ ë‚´ìš© ìŠ¬ë¼ì´ì‹±ì„ ë” ì¤„ì´ê±°ë‚˜, chunk ë°©ì‹ìœ¼ë¡œ ë¶„í•  ìš”ì•½í•˜ëŠ” ê²Œ ì•ˆì „í•´ìš”.")
            elif total_used > 100000:
                st.error("ğŸš¨ ê±°ì˜ í•œê³„ì¹˜ì— ë„ë‹¬í–ˆì–´ìš”. ë¬¸ì„œ ê¸¸ì´ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ê¼­ í•„ìš”í•©ë‹ˆë‹¤.")

        with st.expander(f"ë¶„ì„ ê²°ê³¼", expanded=False):        
            # ê²°ê³¼ ì²˜ë¦¬
            if gpt_response:
                content = gpt_response.choices[0].message.content.strip()
                if content:
                    st.markdown(content)
                else:
                    st.warning("GPT ì‘ë‹µì´ ë¹„ì–´ ìˆì–´ìš”.")
            else:
                st.error("GPT í˜¸ì¶œì— ë°˜ë³µ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")